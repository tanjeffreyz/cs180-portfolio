<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>CS 180 Project 4</title>

    <link rel="stylesheet" href="style.css?">
    <link
        href="https://fonts.googleapis.com/css2?family=Noto+Serif:wght@400;700&family=Open+Sans:ital,wght@0,400;0,700;1,600&display=swap"
        rel="stylesheet">

    <script defer src="https://use.fontawesome.com/releases/v5.7.2/js/all.js"
        integrity="sha384-0pzryjIRos8mFBWMzSSZApWtPl/5++eIfzYmTgBBmXYdhvxPc+XcFEk+zJwDgWbP"
        crossorigin="anonymous"></script>
    <script src="offsets.js"></script>
</head>

<body>
    <!-- Title -->
    <div class="navbar clear nav-top">
        <div class="row" style="text-align: center;">
            <h1>CS 180 Project 4: [Auto]Stitching Photo Mosaics</h1>
            <b>By Jeffrey Tan</b>
        </div>
    </div>

    <div class="container clear">
        <div class="row wrapper">
            <!-- Table of Contents -->
            <div class="sidepanel" id="table-of-contents"></div>

            <!-- Contents -->
            <div class="right-col">
                <div style="text-align: center;">
                    <img src="results/4_mosaic/bangkok.png" width="100%"/>
                </div>



                <h1 id="introduction">Introduction</h1>
                <p>
                    The first part of this project explores how to warp and stitch images together into mosaics using manually labeled correspondence points. This same warping operation can also be used rectify images and fit them to different perspectives. The second part of this project focuses on how to stitch images together into mosaics using correspondence points that are automatically identified, filtered, and matched.
                </p>






                <h1 id="shoot-the-pictures">Part A: Shoot the Pictures</h1>

                <h2 id="shoot-approach">Approach</h2>
                <p>
                  Since I did not have a tripod, I balanced my phone on top of my water bottle and turned the water bottle to simulate a tripod. Long-pressing the screen locked the exposure and focus of the camera, which allowed me to take far more consistent photos and stitch more convincing mosaics.
                </p>

                <h2 id="shoot-failures">Failures</h2>
                <p>
                  At first, I took pictures by holding my phone in front of me and turning my body. While this was still fine for aligning objects far away, the non-zero turn radius meant that the perspectives did not share a common center, and this error was amplified for nearby objects, such as the tree in the mosaic below. The error made aligning these closer objects while keeping the distant objects aligned impossible:
                </p>
                <div class="gallery">
                  <img src="results/1_homography/failure.png"/>
                </div>
                







                <h1 id="homography">Part A: Recover Homographies</h1>
                <h2 id="homography-approach">Approach</h2>
                <p>
                  I used the same <a href="https://inst.eecs.berkeley.edu/~cs194-26/fa22/upload/files/proj3/cs194-26-aex/tool.html">correspondence tool</a> used in Project 3 to label matching features between each pair of images in the mosaic. After obtaining the correspondences <code>src_pts</code> for the source image and <code>trg_pts</code> for the target image, I used the SVD approach described in <a href="https://cseweb.ucsd.edu/classes/wi07/cse252a/homography_estimation/homography_estimation.pdf">this paper</a> to calculate the homography matrix. For each pair of correspondence points <code>(x1, y1)</code> and <code>(x2, y2)</code>, two vectors are created following the formula below:
                </p>
                <div class="gallery">
                  <img src="results/1_homography/homography_equations.png" />
                </div>
                <p>
                  These two vectors are derived from eliminating the scale factor <code>w'</code> in the target homogenous coordinates. Afterwards, these vectors are stacked into a matrix <code>A</code>, which is then passed through <code>np.linalg.svd</code> to obtain <code>S</code>, <code>U</code>, and <code>Vt</code>. The last row <code>v</code> of <code>Vt</code> (last column of <code>V</code>) corresponds with the smallest singular value and is taken to be the elements of the flattened homography matrix. Lastly, <code>v</code> is reshaped into a <code>3x3</code> matrix <code>H</code> and each of its elements is divided by the bottom-right-most element to unscale the transform.
                </p>




                <h1 id="warp">Part A: Warp the Images</h1>
                <h2 id="warp-approach">Approach</h2>
                <p>
                  For each pair of images, after calculating the homography matrix <code>H</code>, the correspondences in the source image <code>src_img</code> are warped to fit those of the target image <code>trg_img</code> using a similar approach to the one used in Project 3. First, the points of bounding box <code>src_bounds</code> represented by the four corners of the image are stacked row-by-row into a matrix of homogeneous coordinates. These bounds are then warped to the bounds of the resulting image by applying the homography matrix like so: <code>trg_bounds = src_bounds @ H</code>. Next, <code>skimage.draw.polygon</code> is used to list all pixel locations <code>trg_pts</code> in the quadrilateral bounded by <code>trg_bounds</code>. For each pixel in <code>trg_pts</code>, the inverse transform <code>H_inv = np.linalg.inv(H)</code> is applied by doing <code>inv_pts = trg_pts @ H_inv</code>. At each point in <code>inv_pts</code>, the neighboring pixel values in the original image <code>src_img</code> are sampled using bilinear interpolation to produce the final pixel value at that pixel in the warped image.
                </p>

                <h2 id="warp-rectify">Rectifying Images</h2>
                <p>
                  In order to rectify an image, four correspondences <code>src_pts</code> on a rectangular object were marked on the input image, and the target correspondences <code>trg_pts</code> were manually determined to form a rectangle in the center of the final rectified image. The warping algorithm described above is then used to warp the input image such that the face of the rectangular object is directly facing the camera.
                </p>
                <table>
                  <tr>
                    <th>Original</th>
                    <th>Rectified</th>
                  </tr>
                  <tr>
                    <td>
                      <img src="data/rectify/macbook.png"/>
                    </td>
                    <td>
                      <img src="results/3_rectify/macbook.png" />
                    </td>
                  </tr>
                  <tr>
                    <td>
                      <img src="data/rectify/monitor.png"/>
                    </td>
                    <td>
                      <img src="results/3_rectify/monitor.png" />
                    </td>
                  </tr>
                </table>





                <h1 id="blend">Part A: Blend the Images into Mosaics</h1>
                <h2 id="blend-approach">Approach</h2>
                <p>
                  In order to produce a mosaic, the images in the mosaic were first organized into a tree such that image <code>A</code> is the parent of <code>B</code> if the correspondences in image <code>B</code> needs to be warped to fit image <code>A</code>. The correspondence points generated from the correspondence tool represents a directed edge from <code>B</code> to <code>A</code>. The root node is the unwarped reference image at the center of the mosaic. The images and correspondence points are loaded while building the tree, and for each node <code>n</code>, the "delta" homography matrix <code>delta_H</code> is computed by using its own correspondence points <code>n.pts</code> and its parents correspondence points <code>n.parent.pts</code>. Then, its final homography matrix <code>n.H</code> is computed by multiplying <code>n.parent.H @ delta_H</code>. This chaining of homography matrices allows the mosaicing of images that are not directly linked by correspondence points. When rendering the mosaic, the nodes in the mosaic tree are processed in BFS order starting at the root so that images towards the edges of the mosaic are overlayed on top of those near the center.
                </p>
                <p>
                  After warping an image to its final orientation within the mosaic, it is blended with the rest of the mosaic using the same Laplacian and Gaussian stack used in Project 2. The irregular mask used in the blend is derived from the final shape of the warped image and the blend is performed with <code>num_levels=2</code> and <code>kernel_size=25</code> with <code>sigma</code> scaled accordingly.
                </p>

                <h2 id="blend-shortcomings">Shortcomings</h2>
                <p>
                  While the tree approach to mosaicing sounds good in theory, floating point error and rounding to discrete pixel positions causes error to accumulate. This results in the images further out near the edges of the mosaic to fall slightly out of alignment, even though their pair-wise alignments were verified to be correct. This is something I want to look into further in Part B of this project, but due to time constraints, I am sticking with mosaics of at most 3 images for now.
                </p>


                <h2 id="blend-university">University Hall</h2>
                <table>
                  <tr>
                    <th>Root Image</th>
                    <th>Right Leaf</th>
                  </tr>
                  <tr>
                    <td>
                      <img src="data/university_hall/IMG_4651.png" />
                    </td>
                    <td>
                      <img src="data/university_hall/IMG_4652.png" />
                    </td>
                  </tr>
                </table>
                <table>
                  <tr>
                    <th>Mosaic</th>
                  </tr>
                  <tr>
                    <td>
                      <img src="results/4_mosaic/university_hall.png" />
                    </td>
                  </tr>
                </table>

                <h2 id="blend-home">Home</h2>
                <table>
                  <tr>
                    <th>Left Leaf</th>
                    <th>Root Image</th>
                  </tr>
                  <tr>
                    <td>
                      <img src="data/home/IMG_4658.png" />
                    </td>
                    <td>
                      <img src="data/home/IMG_4659.png" />
                    </td>
                  </tr>
                </table>
                <table>
                  <tr>
                    <th>Mosaic</th>
                  </tr>
                  <tr>
                    <td>
                      <img src="results/4_mosaic/home.png" />
                    </td>
                  </tr>
                </table>


                <h2 id="blend-bangkok">Bangkok Noodles & Thai BBQ</h2>
                <table>
                  <tr>
                    <th>Left Leaf</th>
                    <th>Root Image</th>
                    <th>Right Leaf</th>
                  </tr>
                  <tr>
                    <td>
                      <img src="data/bangkok/IMG_4653.png" />
                    </td>
                    <td>
                      <img src="data/bangkok/IMG_4654.png" />
                    </td>
                    <td>
                      <img src="data/bangkok/IMG_4655.png" />
                    </td>
                  </tr>
                </table>
                <table>
                  <tr>
                    <th>Mosaic</th>
                  </tr>
                  <tr>
                    <td>
                      <img src="results/4_mosaic/bangkok.png" />
                    </td>
                  </tr>
                </table>













                



                <h1 id="detect-corners">Part B: Detecting Corners</h1>
                <h2 id="detect-corners-approach">Approach</h2>
                <p>
                    First, all <code>corners</code> in the image are detected using the Harris corner detection algorithm, and the scores <code>h</code> for each pixel in the image is retrieved using <code>get_harris_corners()</code>. The scores for each corner are extracted into a vector <code>scores = h[corners[:, 0], corners[:, 1]]</code>. Adaptive Non-Maximal Suppression (ANMS) is then used to identify the strongest corners in the image while still keeping the points spread out relatively evenly across the image. To perform ANMS, the parwise distances <code>dists</code> are calculated using <code>dist2()</code>. Numpy broadcasting is then used to determine whether <code>f(x_i) < c_robust * f(x_j)</code> holds true for each pair of corners, resulting in a mask <code>larger_mask = scores[:, np.newaxis] < (c_robust * scores[np.newaxis, :])</code>. Then, <code>dists</code> is masked using this mask to produce <code>masked_dists = dists * larger_mask</code>, and the distances between pairs of corners that do not satisfy the inequality above are set to infinity so that they do not take part in the upcoming minimization. This way, the minimum radii for each point can be calculated using <code>radii = np.min(masked_dists, axis=1)</code>. Next, the indices of each point are sorted in order of decreasing radii using <code>sorted_indices = (-radii).argsort()</code>. Finally, these indices are used to sort the original list of corners using <code>sorted_corners = corners[sorted_indices]</code> and the best corners are returned using <code>points = sorted_corners[:num_corners]</code>.
                </p>

                <h2 id="detect-results">Results</h2>
                <table>
                    <tr>
                        <th>All Corners</th>
                        <th>ANMS Corners</th>
                    </tr>
                    <tr>
                        <td>
                            <img src="results/part1/all_corners.png"/>
                        </td>
                        <td>
                            <img src="results/part1/top_500_corners.png"/>
                        </td>
                    </tr>
                </table>





                <h1 id="extracting">Part B: Extracting Feature Descriptors</h1>

                <h2 id="extracting-approach">Approach</h2>
                <p>
                    For each corner in <code>points</code> obtained from ANMS, a <code>40x40</code> region <code>feature</code> around the given point is sliced from the original color image, which is then resized to <code>8x8</code> using <code>skimage.transform.resize</code>. The feature is then normalized by subtracting the mean and dividing by the standard deviation for each of the three channels. Lastly it is flattened and stacked into a <code>Nx192</code> matrix with each row being a flattened feature descriptor. This is performed for each image in the pair to produce <code>features1</code> and <code>features2</code>.
                </p>


                <h2 id="extracting-results">Results</h2>
                <p>
                    Here are some examples of the extracted <code>8x8</code> feature descriptors:
                </p>
                <div class="gallery">
                    <img src="results/part2/feature_1.png"/>
                    <img src="results/part2/feature_2.png"/>
                    <img src="results/part2/feature_3.png"/>
                    <img src="results/part2/feature_4.png"/>
                    <img src="results/part2/feature_5.png"/>
                    <img src="results/part2/feature_6.png"/>
                </div>





                <h1 id="matching">Part B: Matching Feature Descriptors</h1>

                <h2 id="matching-approach">Approach</h2>
                <p>
                    The pairwise differences between each flattened feature descriptor is calculated using <code>diff = features1[:, np.newaxis, :] - features2[np.newaxis, :, :]</code>, and the pairwise sum-squared differences (SSD) are calculated by summing over the last dimension of size <code>192</code> using <code>ssd = np.sum(diff ** 2, axis=-1)</code>. The nearest-neighbor distances are then sorted within each row (representing a feature of interest in the first image) from smallest distance to largest. The Lowe score for each feature is calculated using the 1-NN and 2-NN distances in <code>ssd</code> through <code>lowe = nn_dists[:, 0] / nn_dists[:, 1]</code>. All features with a Lowe score lower than the threshold is extracted using the mask <code>lowe_mask = lowe < lowe_threshold</code>, which will be used to filter the matches at the end. The indices of the closest nearest neighbor for each feature is retrieved using <code>closest = ssd.argsort()[:, 0]</code>. These indices are then paired with the indices of the first image's features using <code>matches = np.stack([np.arange(0, closest.shape[0]), closest]).T</code>. Lastly, the pairings are filtered using the mask from before: <code>matches[lowe_mask]</code>.
                </p>


                <h2 id="matching-results">Results</h2>
                <p>
                    Here are some examples of matches between feature descriptors:
                </p>
                <div class="gallery">
                    <img src="results/part3/match_1.png"/>
                    <img src="results/part3/match_2.png"/>
                    <img src="results/part3/match_3.png"/>
                    <img src="results/part3/match_4.png"/>
                    <img src="results/part3/match_5.png"/>
                    <img src="results/part3/match_6.png"/>
                </div>

                <p>
                    This matching process resulted in the following correspondences, which were obtained by indexing into the original list of detected points using <code>corr1 = points1[matches[:, 0]]</code> and <code>corr2 = points2[matches[:, 1]]</code>.
                </p>
                <div>
                    <img src="results/part3/home_correspondences.png"/ width="100%">
                </div>




                <h1 id="ransac">Part B: Random Sample Consensus (RANSAC)</h1>
                
                <h2 id="ransac-approach">Approach</h2>
                <p>
                    RANSAC is implemented and used in the <code>Mosaic</code> class from Part A of this project. Given a set of <code>src_pts</code> and <code>trg_pts</code>, during each iteration of the algorithm, the indices of four points from <code>src_pts</code> are sampled without replacement using <code>indices = np.random.choice(src_pts.shape[0], size=4, replace=False)</code> and the points are retrieved through indexing using <code>src_sample = src_pts[indices, :]</code> and <code>trg_sample = trg_pts[indices, :]</code>. Using these samples, the homography is calculated exactly like in Part A using <code>h = compute_homography(src_sample, trg_sample)</code>. Then, all the source points are transformed using <code>transformed_pts = src_pts @ h</code> and unscaled. The Euclidean distance between the transformed points and the ground truth targets are calculated using <code>dists = np.sqrt(np.sum((transformed_pts - trg_pts) ** 2, axis=-1))</code>. Lastly, all points that land within a given threshold of their targets are extracted using a mask <code>mask = (dists < threshold)</code>. The number of points that are under the threshold is counted by summing the mask. If that count is larger than the size of the current largest set of points, update the largest set of points to be <code>best_src = src_pts[mask]</code> and <code>best_trg = trg_pts[mask]</code>. After all the iterations, <code>best_src</code> and <code>best_trg</code> are returned. These points are then passed into the <code>compute_homography</code> function from Part A to calculate the homography matrix.
                </p>

                <h2 id="ransac-results">Results</h2>
                <p>
                    The RANSAC inliers for each pair of correspondences are shown below:
                </p>
                <div>
                    <img src="results/part4/home_inliers.png" width="100%"/>
                    <img src="results/part4/university_hall_inliers.png" width="100%"/>
                    <img src="results/part4/bangkok_inliers_left.png" width="100%"/>
                    <img src="results/part4/bangkok_inliers_right.png" width="100%"/>
                </div>


                <h1 id="mosaic">Part B: Autostitching Mosaics</h1>

                <h2 id="mosaic-approach">Approach</h2>
                <p>
                    The automatically generated correspondences are stored in the same JSON file format used by <code>Mosaic</code> from Part A, and the code is run as-is with the exception that the homographies are now calculated using RANSAC.
                </p>

                <h2 id="mosaic-results">Results</h2>
                <p>
                    Overall, the autostitching performed almost as well on the mosaics from Part A. The autostitched mosaics were a couple pixels more inaccurate than my manually labeled correspondences, but this is most likely due to the fact that I took the time to iteratively choose manual correspondences to get the best alignment I could. Another factor is that, with autostitching, the user has little control over the features it chooses. This is evident in how the roof of University Hall is slightly misaligned in the autostitched mosaic. Additionally, the two chairs in the middle of the autostitched restaurant mosaic are slightly misaligned and so is the far wall.
                </p>

                <p>
                    Interestingly, the autostitched mosaic of my living room was slightly better than the manual version. The floorboards near the center of the mosaic are aligned more accurately in the autostitched version, which makes sense because I had trouble manually finding the right corners on the floor to align the patterns in the floorboards, whereas it is probably easier for the algorithm to find more accurate matches between the irregular patterns.
                </p>
                <table>
                    <tr>
                        <th>Manual</th>
                        <th>Autostitched</th>
                    </tr>
                    <tr>
                        <td>
                            <img src="results/part5/originals/university_hall.png"/>
                        </td>
                        <td>
                            <img src="results/part5/university_hall.png"/>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <img src="results/part5/originals/home.png"/>
                        </td>
                        <td>
                            <img src="results/part5/home.png"/>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <img src="results/part5/originals/bangkok.png"/>
                        </td>
                        <td>
                            <img src="results/part5/bangkok.png"/>
                        </td>
                    </tr>
                </table>



                <h1 id="reflection">Reflection</h1>
                <p>
                    Visualizing how the forward and inverse homography matrices transform the input was tricky but fun, and it was interesting to learn that homography matrices can be chained together to link images that do not share any correspondences at all.
                    The feature descriptor matching was really interesting. Implementing the pairwise SSD using broadcasting instead of loops was a fun challenge. It was also interesting visualizing the final features that the algorithm chose, since for the most part, they weren't the features that I initially focused on.
                </p>

        </div>
    </div>

    <!-- Populate table of contents -->
    <script>
        let first = true
        const toc = document.getElementById('table-of-contents')
        const headings = document.querySelectorAll('h1, h2, h3, h4, h5, h6')
        for (let i = 0; i < headings.length; ++i) {
            const node = headings[i]

            // If no id, nothing to link to so skip
            if (!node.id) continue
            
            // Different class for h1, h2, etc
            const tag = node.tagName.toLowerCase()
            let linkClassName = ''
            if (tag === 'h1') {
                // Don't add divider before first item
                if (first) {
                    first = false
                } else {
                    // Add ToC divider
                    const divider = document.createElement('div')
                    divider.classList.add('divider')
                    toc.appendChild(divider)

                    // Add section divider in page body
                    const sectionDivider = document.createElement('div')
                    sectionDivider.classList.add('section-divider')
                    node.parentNode.insertBefore(sectionDivider, node)
                }
                linkClassName = 'title'
            } else if (tag === 'h2') {
                linkClassName = 'section'
            }

            // Add link with correct class/formatting and same text content as
            // its corresponding heading
            const link = document.createElement('a')
            link.classList.add(linkClassName)
            link.href = '#' + node.id
            link.innerHTML = node.innerHTML
            toc.appendChild(link)
        }
    </script>
</body>

</html>
